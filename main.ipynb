{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformer\n",
    "\n",
    "Following https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras_cv\n",
    "import keras\n",
    "from glob import glob\n",
    "print(\"Keras:\", keras.__version__)\n",
    "print(\"KerasCV:\", keras_cv.__version__)\n",
    "import joblib\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "    \n",
    "    # initialize dimensions\n",
    "    self.d_model = d_model # model dimension\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = d_model // num_heads # dimension of each head's key, query, and value\n",
    "    \n",
    "    # Linear layers\n",
    "    self.W_q = nn.Linear(d_model, d_model) # Query\n",
    "    self.W_k = nn.Linear(d_model, d_model) # Key\n",
    "    self.W_v = nn.Linear(d_model, d_model) # Value\n",
    "    self.W_o = nn.Linear(d_model, d_model) # Output\n",
    "  \n",
    "  def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "    # calculate attention scores\n",
    "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    \n",
    "    # apply mask if provided\n",
    "    if mask is not None:\n",
    "      attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # softmax to get attention probabilities\n",
    "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "    # multiply by values to obtain output\n",
    "    output = torch.matmul(attn_probs, V)\n",
    "    return output\n",
    "  \n",
    "  def split_heads(self, x):\n",
    "    # reshape input for num_heads\n",
    "    batch_size, seq_length, d_model = x.size()\n",
    "    return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "  \n",
    "  def combine_heads(self, x):\n",
    "    # combine multiple heads back to original shape\n",
    "    batch_size, _, seq_length, d_k = x.size()\n",
    "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "  \n",
    "  def forward(self, Q, K, V, mask=None):\n",
    "    # Linear transformations and split heads\n",
    "    Q = self.split_heads(self.W_q(Q))\n",
    "    K = self.split_heads(self.W_k(K))\n",
    "    V = self.split_heads(self.W_v(V))\n",
    "    \n",
    "    # scaled dot-product attention\n",
    "    attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "    \n",
    "    # Combine heads and apply transformation\n",
    "    output = self.W_o(self.combine_heads(attn_output))\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "  def __init__(self, d_model, d_ff):\n",
    "    super(PositionWiseFeedForward, self).__init__()\n",
    "    self.fc1 = nn.Linear(d_model, d_ff)\n",
    "    self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    self.relu = nn.ReLU()\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "    \n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x, mask):\n",
    "    attn_output = self.self_attn(x, x, x, mask)\n",
    "    x = self.norm1(x + self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm2(x + self.dropout(ff_output))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "    self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.norm3 = nn.LayerNorm(d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "    attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "    x = self.norm1(x + self.dropout(attn_output))\n",
    "    attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "    x = self.norm2(x + self.dropout(attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm3(x + self.dropout(ff_output))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        src_vocab_size: source vocabulary size\n",
    "        tgt_vocab_size: target vocabulary size\n",
    "        d_model: The dimensionality of the model's embeddings\n",
    "        num_heads: Number of attention heads in the multi-head attention mechanism\n",
    "        num_layers: Number of layers for both the encoder and decoder\n",
    "        d_ff: Dimensionality of the inner layer in the feed-forward network\n",
    "        max_seq_length: Maximum sequence length for positional encoding\n",
    "        dropout: Dropout rate for regularization\n",
    "    \"\"\"\n",
    "    super(Transformer, self).__init__()\n",
    "    self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "    self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "    self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "    \n",
    "    self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "    self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "    \n",
    "    self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    self.to_latent = nn.Identity()\n",
    "    \n",
    "    self.mlp_head = nn.Linear()\n",
    "  \n",
    "  def generate_mask(self, src, tgt):\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "    seq_length = tgt.size(1)\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "    tgt_mask = tgt_mask & nopeak_mask\n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "  def forward(self, src, tgt):\n",
    "    src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "    src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "    tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "    enc_output = src_embedded\n",
    "    for enc_layer in self.encoder_layers:\n",
    "        enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "    dec_output = tgt_embedded\n",
    "    for dec_layer in self.decoder_layers:\n",
    "        dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "    output = self.fc(dec_output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    verbose = 1  # Verbosity\n",
    "    seed = 42  # Random seed\n",
    "    image_size = [400, 300]  # Input image size\n",
    "    epochs = 13 # Training epochs\n",
    "    batch_size = 64  # Batch size\n",
    "    drop_remainder = True  # Drop incomplete batches\n",
    "    num_classes = 6 # Number of classes in the dataset\n",
    "    fold = 0 # Which fold to set as validation data\n",
    "    class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n",
    "    label2name = dict(enumerate(class_names))\n",
    "    name2label = {v:k for k, v in label2name.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"kaggle/input/hms-harmful-brain-activity-classification\"\n",
    "\n",
    "SPEC_DIR = \"tmp/dataset/hms-hbac\"\n",
    "os.makedirs(SPEC_DIR+'/train_spectrograms', exist_ok=True)\n",
    "os.makedirs(SPEC_DIR+'/test_spectrograms', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train + Valid\n",
    "df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "df['eeg_path'] = f'{BASE_PATH}/train_eegs/'+df['eeg_id'].astype(str)+'.parquet'\n",
    "df['spec_path'] = f'{BASE_PATH}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.parquet'\n",
    "df['spec2_path'] = f'{SPEC_DIR}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.npy'\n",
    "df['class_name'] = df.expert_consensus.copy()\n",
    "df['class_label'] = df.expert_consensus.map(CFG.name2label)\n",
    "display(df.head(2))\n",
    "\n",
    "# Test\n",
    "test_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n",
    "test_df['eeg_path'] = f'{BASE_PATH}/test_eegs/'+test_df['eeg_id'].astype(str)+'.parquet'\n",
    "test_df['spec_path'] = f'{BASE_PATH}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.parquet'\n",
    "test_df['spec2_path'] = f'{SPEC_DIR}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.npy'\n",
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process a single eeg_id\n",
    "def process_spec(spec_id, split=\"train\"):\n",
    "    spec_path = f\"{BASE_PATH}/{split}_spectrograms/{spec_id}.parquet\"\n",
    "    spec = pd.read_parquet(spec_path)\n",
    "    spec = spec.fillna(0).values[:, 1:].T # fill NaN values with 0, transpose for (Time, Freq) -> (Freq, Time)\n",
    "    spec = spec.astype(\"float32\")\n",
    "    np.save(f\"{SPEC_DIR}/{split}_spectrograms/{spec_id}.npy\", spec)\n",
    "\n",
    "# Get unique spec_ids of train and valid data\n",
    "spec_ids = df[\"spectrogram_id\"].unique()\n",
    "\n",
    "# Parallelize the processing using joblib for training data\n",
    "_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    joblib.delayed(process_spec)(spec_id, \"train\")\n",
    "    for spec_id in tqdm(spec_ids, total=len(spec_ids))\n",
    ")\n",
    "\n",
    "# Get unique spec_ids of test data\n",
    "test_spec_ids = test_df[\"spectrogram_id\"].unique()\n",
    "\n",
    "# Parallelize the processing using joblib for test data\n",
    "_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    joblib.delayed(process_spec)(spec_id, \"test\")\n",
    "    for spec_id in tqdm(test_spec_ids, total=len(test_spec_ids))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmenter(dim=CFG.image_size):\n",
    "    augmenters = [\n",
    "        keras_cv.layers.MixUp(alpha=2.0),\n",
    "        keras_cv.layers.RandomCutout(height_factor=(1.0, 1.0),\n",
    "                                     width_factor=(0.06, 0.1)), # freq-masking\n",
    "        keras_cv.layers.RandomCutout(height_factor=(0.06, 0.1),\n",
    "                                     width_factor=(1.0, 1.0)), # time-masking\n",
    "    ]\n",
    "    \n",
    "    def augment(img, label):\n",
    "        data = {\"images\":img, \"labels\":label}\n",
    "        for augmenter in augmenters:\n",
    "            if tf.random.uniform([]) < 0.5:\n",
    "                data = augmenter(data, training=True)\n",
    "        return data[\"images\"], data[\"labels\"]\n",
    "    \n",
    "    return augment\n",
    "\n",
    "\n",
    "def build_decoder(with_labels=True, target_size=CFG.image_size, dtype=32):\n",
    "    def decode_signal(path, offset=None):\n",
    "        # Read .npy files and process the signal\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        sig = tf.io.decode_raw(file_bytes, tf.float32)\n",
    "        sig = sig[1024//dtype:]  # Remove header tag\n",
    "        sig = tf.reshape(sig, [400, -1])\n",
    "        \n",
    "        # Extract labeled subsample from full spectrogram using \"offset\"\n",
    "        if offset is not None: \n",
    "            offset = offset // 2  # Only odd values are given\n",
    "            sig = sig[:, offset:offset+300]\n",
    "            \n",
    "            # Pad spectrogram to ensure the same input shape of [400, 300]\n",
    "            pad_size = tf.math.maximum(0, 300 - tf.shape(sig)[1])\n",
    "            sig = tf.pad(sig, [[0, 0], [0, pad_size]])\n",
    "            sig = tf.reshape(sig, [400, 300])\n",
    "        \n",
    "        # Log spectrogram \n",
    "        sig = tf.clip_by_value(sig, tf.math.exp(-4.0), tf.math.exp(8.0)) # avoid 0 in log\n",
    "        sig = tf.math.log(sig)\n",
    "        \n",
    "        # Normalize spectrogram\n",
    "        sig -= tf.math.reduce_mean(sig)\n",
    "        sig /= tf.math.reduce_std(sig) + 1e-6\n",
    "        \n",
    "        # Mono channel to 3 channels to use \"ImageNet\" weights\n",
    "        sig = tf.tile(sig[..., None], [1, 1, 3])\n",
    "        return sig\n",
    "    \n",
    "    def decode_label(label):\n",
    "        label = tf.one_hot(label, CFG.num_classes)\n",
    "        label = tf.cast(label, tf.float32)\n",
    "        label = tf.reshape(label, [CFG.num_classes])\n",
    "        return label\n",
    "    \n",
    "    def decode_with_labels(path, offset=None, label=None):\n",
    "        sig = decode_signal(path, offset)\n",
    "        label = decode_label(label)\n",
    "        return (sig, label)\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode_signal\n",
    "\n",
    "\n",
    "def build_dataset(paths, offsets=None, labels=None, batch_size=32, cache=True,\n",
    "                  decode_fn=None, augment_fn=None,\n",
    "                  augment=False, repeat=True, shuffle=1024, \n",
    "                  cache_dir=\"\", drop_remainder=False):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None)\n",
    "    \n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter()\n",
    "    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = (paths, offsets) if labels is None else (paths, offsets, labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    ds = ds.cache(cache_dir) if cache else ds\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "df[\"fold\"] = -1\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(\n",
    "    sgkf.split(df, y=df[\"class_label\"], groups=df[\"patient_id\"])\n",
    "):\n",
    "    df.loc[valid_idx, \"fold\"] = fold\n",
    "df.groupby([\"fold\", \"class_name\"])[[\"eeg_id\"]].count().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from full data\n",
    "sample_df = df.groupby(\"spectrogram_id\").head(1).reset_index(drop=True)\n",
    "train_df = sample_df[sample_df.fold != CFG.fold]\n",
    "valid_df = sample_df[sample_df.fold == CFG.fold]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "# Train\n",
    "train_paths = train_df.spec2_path.values\n",
    "train_offsets = train_df.spectrogram_label_offset_seconds.values.astype(int)\n",
    "train_labels = train_df.class_label.values\n",
    "train_ds = build_dataset(train_paths, train_offsets, train_labels, batch_size=CFG.batch_size,\n",
    "                         repeat=True, shuffle=True, augment=True, cache=True)\n",
    "\n",
    "# Valid\n",
    "valid_paths = valid_df.spec2_path.values\n",
    "valid_offsets = valid_df.spectrogram_label_offset_seconds.values.astype(int)\n",
    "valid_labels = valid_df.class_label.values\n",
    "valid_ds = build_dataset(valid_paths, valid_offsets, valid_labels, batch_size=CFG.batch_size,\n",
    "                         repeat=False, shuffle=False, augment=False, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf\n",
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
